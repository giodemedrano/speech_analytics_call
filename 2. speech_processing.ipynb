{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import statistics\n",
    "import os\n",
    "import logging\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.util import ngrams, bigrams\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px #Graficos interactivos\n",
    "\n",
    "mypath = 'Voicebot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.handlers.WatchedFileHandler(os.environ.get(\"LOGFILE\", f\"{mypath}/debug.log\"))\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root = logging.getLogger()\n",
    "root.setLevel(os.environ.get(\"LOGLEVEL\", \"INFO\"))\n",
    "root.addHandler(handler)\n",
    "logging.info(f\"INICIO PROCESO '2.speech_processing.ipynb'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir(f'{mypath}/transcripts/json/') if isfile(join(f'{mypath}/transcripts/json/', f))]\n",
    "logging.info(f\"{len(onlyfiles)} files inside '{mypath}/transcripts/json'\")\n",
    "jsonfiles = [os.path.splitext(filename)[0] for filename in onlyfiles if os.path.splitext(filename)[1]=='.json']\n",
    "logging.info(f\"{len(onlyfiles)} jsonfiles inside '{mypath}/transcripts/json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 files appended to rawdata\n",
      "100 times extracted from 'Voicebot/transcripts/info/times.json'\n"
     ]
    }
   ],
   "source": [
    "# load .json\n",
    "rawdata = []\n",
    "rawinfo = []\n",
    "\n",
    "for jsonfile in jsonfiles:\n",
    "  with open(f\"{mypath}/transcripts/json/{jsonfile}.json\", encoding='utf8') as json_file:\n",
    "    rawdata.append(json.load(json_file)) # Transcripts and data\n",
    "\n",
    "with open(f\"{mypath}/transcripts/info/times.json\", encoding='utf8') as json_file:\n",
    "  for times in json.load(json_file):\n",
    "    rawinfo.append(times) # Time of processing transcriptions\n",
    "  \n",
    "# data = rawdata[0]\n",
    "logging.info(f\"{len(rawdata)} files append to rawdata\")\n",
    "print(f\"{len(rawdata)} files appended to rawdata\")\n",
    "logging.info(f\"{len(rawdata)} times extracted from '{mypath}/transcripts/info/times.json'\")\n",
    "print(f\"{len(rawdata)} times extracted from '{mypath}/transcripts/info/times.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.keys()                       # dict_keys(['audio_metrics', 'results', 'result_index', 'speaker_labels'])\n",
    "## data['audio_metrics'].keys()     # dict_keys(['sampling_interval', 'accumulated'])\n",
    "## data['results'][0].keys()        # dict_keys(['alternatives', 'final'])\n",
    "## data['result_index']             # 0\n",
    "## data['speaker_labels'][0].keys() # dict_keys(['from', 'to', 'speaker', 'confidence', 'final'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info data: audio metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  data['audio_metrics'].keys()                                    # dict_keys(['sampling_interval', 'accumulated'])\n",
    "###                      # 0.1 [seconds] *\n",
    "### data['audio_metrics']['accumulated'].keys()                     # dict_keys(['non_speech_level', 'clipping_rate', 'high_frequency_loss', 'end_time', 'speech_ratio',                                                                                      'direct_current_offset','final', 'signal_to_noise_ratio', 'speech_level'])\n",
    "### data['audio_metrics']['accumulated']['non_speech_level']        # Non-Speech levels *                \n",
    "### data['audio_metrics']['accumulated']['clipping_rate']           # Clipping rates\n",
    "### data['audio_metrics']['accumulated']['high_frequency_loss']     # 0.0\n",
    "### data['audio_metrics']['accumulated']['end_time']                # 147.6 [seconds] *\n",
    "### data['audio_metrics']['accumulated']['speech_ratio']            # 0.732 [speech_time/end_time] *\n",
    "### data['audio_metrics']['accumulated']['direct_current_offset']   # Offset levels\n",
    "### data['audio_metrics']['accumulated']['final']                   # True\n",
    "### data['audio_metrics']['accumulated']['signal_to_noise_ratio']   # 12.8 *\n",
    "### data['audio_metrics']['accumulated']['speech_level']            # Speech levels *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Bads Requests...\n",
    "bads = []\n",
    "for i,data in enumerate(rawdata):\n",
    "    try: \n",
    "        data['error']\n",
    "        bads.append(i)\n",
    "    except:\n",
    "        pass\n",
    "if bads == []:\n",
    "    logging.info(f\"No bad requests transcripts found\")\n",
    "else: logging.warning(f\"Bad requests found: {bads}\")\n",
    "bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_metrics = []\n",
    "for i, data in enumerate(rawdata):\n",
    "    sampling_interval       = data['audio_metrics']['sampling_interval']\n",
    "    non_speech_level_freq   = [non_speech_level['count'] for non_speech_level in data['audio_metrics']['accumulated']['non_speech_level']]\n",
    "    non_speech_level_mean   = sum(freq*(i + 0.5)/10 for i, freq in enumerate(non_speech_level_freq))/sum(non_speech_level_freq) if sum(non_speech_level_freq) != 0 else 0\n",
    "    non_speech_level_var    = sum(freq*((i + 0.5)/10 - non_speech_level_mean)**2 for i, freq in enumerate(non_speech_level_freq))/sum(non_speech_level_freq) if sum(non_speech_level_freq) != 0 else 0\n",
    "    end_time                = data['audio_metrics']['accumulated']['end_time']\n",
    "    speech_ratio            = data['audio_metrics']['accumulated']['speech_ratio']\n",
    "    final                   = data['audio_metrics']['accumulated']['final']\n",
    "    # signal_to_noise_ratio   = data['audio_metrics']['accumulated']['signal_to_noise_ratio']\n",
    "    speech_level_freq       = [speech_level['count'] for speech_level in data['audio_metrics']['accumulated']['speech_level']]\n",
    "    speech_level_mean       = sum(freq*(i + 0.5)/10 for i, freq in enumerate(speech_level_freq))/sum(speech_level_freq) if sum(speech_level_freq) != 0 else 0\n",
    "    speech_level_var        = sum(freq*((i + 0.5)/10 - speech_level_mean)**2 for i, freq in enumerate(speech_level_freq))/sum(speech_level_freq) if sum(speech_level_freq) != 0 else 0\n",
    "    audio_metrics.append({  'file_id'               : i + 1,\n",
    "                            'path'                  : r\"\\\\192.9.100.44\\grabaciones\\CAT\\20210427\",\n",
    "                            'file_name'             : f\"{jsonfiles[i]}.ogg\",\n",
    "                            # 'time_transcription'    : rawinfo[i],                      \n",
    "                            'sampling_interval'     : sampling_interval,\n",
    "                            'has_ending'            : final,\n",
    "                            'speech_level_mean'     : speech_level_mean,\n",
    "                            'speech_level_var'      : speech_level_var,\n",
    "                            'non_speech_level_mean' : non_speech_level_mean,\n",
    "                            'non_speech_level_var'  : non_speech_level_var,\n",
    "                            # 'signal_to_noise_ratio' : signal_to_noise_ratio,\n",
    "                            'speech_ratio'          : speech_ratio,\n",
    "                            'time_call'             : end_time,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info data: results & speaker labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  data['results'][0]                                          # List of 'transcripts' divided according to pauses *\n",
    "##  data['results'][0].keys()                                   # dict_keys(['alternatives', 'final'])\n",
    "### data['results'][0]['alternatives'][0]                       # List of 'alternatives' to a specific 'transcript' *\n",
    "### data['results'][0]['alternatives'][0].keys()                # dict_keys(['timestamps', 'confidence', 'transcript', 'word_confidence']) \n",
    "### data['results'][0]['alternatives'][0]['timestamps'][0]      # List of 'timestamps' for each 'word' in 'transcript' with 'begin' and 'end'\n",
    "### data['results'][0]['alternatives'][0]['confidence']         # Value of 'transcript' 'confidence'\n",
    "### data['results'][0]['alternatives'][0]['transcript']         # Value of 'transcript'\n",
    "### data['results'][0]['alternatives'][0]['word_confidence'][0] # List of 'word_confidence' for each 'word' in 'transcript' with confidence value\n",
    "### data['speaker_labels'][0]                                   # List of 'speaker_labels' divided for timestamps\n",
    "### data['speaker_labels'][0].keys()                            # dict_keys(['from', 'to', 'speaker', 'confidence', 'final'])\n",
    "### data['speaker_labels'][0]['from']                           # Value of 'from' timestamp\n",
    "### data['speaker_labels'][0]['to']                             # Value of 'to' timestamp\n",
    "### data['speaker_labels'][0]['speaker']                        # Value of 'speaker' number\n",
    "### data['speaker_labels'][0]['confidence']                     # Value of 'confidence' number\n",
    "### data['speaker_labels'][0]['final']                          # True/False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corporea y Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmkeys = ['word','begin','end','word_confidence','speaker','speaker_confidence','file_id'] # corpus_multi_keys\n",
    "corporea = []               # List of corpus (list of 'sentences' containing separated 'words' in a phone conversation)\n",
    "corporea_multi = []         # List of corpus_multi (list of 'sentences' containing separated 'words', 'timestamps', 'confidence', and 'speaker' in a phone conversation)\n",
    "corporea_multi_dict = []    # List of corpus_multi dictionaries (with labels)\n",
    "for n, data in enumerate(rawdata):\n",
    "    corpus = [[word[0] for word in alternative['timestamps']] for result in data['results'] for alternative in result['alternatives'][:1]]\n",
    "    corpus_flat = [group for sentence in corpus for group in sentence]\n",
    "    corpus_timestamps = [[group_times for group_times in alternative['timestamps']] for result in data['results'] for alternative in result['alternatives'][:1]]\n",
    "    corpus_confidence = [[group_confidence for group_confidence in alternative['word_confidence']] for result in data['results'] for alternative in result        ['alternatives'][:1]]\n",
    "    corpus_speaker = [data['speaker_labels'][sum(len(corpus[i]) for i in range(j)):sum(len(corpus[i]) for i in range(j+1))] for j, sentence in enumerate(corpus)]\n",
    "    corpus_extra = [[(group + [corpus_confidence[i][j][-1]]) for j, group in enumerate(corpus_timestamps[i])] for i, sentence in enumerate(corpus_timestamps)]\n",
    "    corpus_multi = [[(group + [corpus_speaker[i][j]['speaker']] + [corpus_speaker[i][j]['confidence']] + [n+1]) for j, group in enumerate(corpus_extra[i])] for i, sentence in enumerate(corpus_extra)]\n",
    "    corpus_multi_flat = [group for sentence in corpus_multi for group in sentence]\n",
    "    corpus_multi_dict = [[dict(zip(cmkeys, word_group)) for word_group in sentence] for sentence in corpus_multi]\n",
    "    corpus_multi_dict_flat = [group for sentence in corpus_multi_dict for group in sentence]\n",
    "    corporea.append(corpus_flat)\n",
    "    corporea_multi.append(corpus_multi_flat)\n",
    "    corporea_multi_dict.append(corpus_multi_dict_flat)\n",
    "# corporea_multi_dict[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Speaker_f', 'Speaker_s', 'Speaker_0', 'Speaker_1', 'Speaker_2', 'Speaker_3']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_speakers = []\n",
    "for corpus_multi_dict in corporea_multi_dict:\n",
    "    for word in corpus_multi_dict:\n",
    "        if word['speaker'] not in total_speakers: total_speakers.append(word['speaker'])\n",
    "speaker_label = [\"Speaker_f\", \"Speaker_s\"]\n",
    "for i in range(len(total_speakers)): \n",
    "    speaker_label.append(f'Speaker_{i}')\n",
    "speaker_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-3022cab979a6>:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df = df.append(pd.Series(), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.DataFrame(columns=speaker_label)\n",
    "\n",
    "for i,corpus_multi_dict in enumerate(corporea_multi_dict):\n",
    "    try:\n",
    "        words_counter = [0]*len(total_speakers)\n",
    "        first = corpus_multi_dict[0]['speaker']\n",
    "        second = first\n",
    "        for word in corpus_multi_dict:\n",
    "            if first == second:\n",
    "                if first != word['speaker']: \n",
    "                    second = word['speaker']\n",
    "            for id in range(len(words_counter)):\n",
    "                if word['speaker']==id: \n",
    "                    words_counter[id] += 1\n",
    "                    break\n",
    "                    \n",
    "        aux = {speaker_label[0]: first, speaker_label[1]: second}\n",
    "        for j in range(len(total_speakers)): \n",
    "            aux[speaker_label[j+2]] = words_counter[j]\n",
    "        \n",
    "        df = df.append(aux, ignore_index=True)\n",
    "    except:\n",
    "        df = df.append(pd.Series(), ignore_index=True)\n",
    "        pass\n",
    "        \n",
    "df.index += 1\n",
    "dft = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAT', '009810255-8', '20210427', '101607', 'taisagap']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonfiles[0].split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO Extractions from filename\n",
    "# agent_result = [filename.split('_')[-1] for filename in jsonfiles] # The result is extracted from the filename if the file has it\n",
    "agent_name = [filename.split('_')[4] for filename in jsonfiles] # The result is extracted from the filename\n",
    "audio_cartera = [filename.split('_')[0] for filename in jsonfiles]\n",
    "audio_fecha = [filename.split('_')[2] for filename in jsonfiles]\n",
    "audio_hora = [filename.split('_')[3] for filename in jsonfiles]\n",
    "rut = [filename.split('_')[1] for filename in jsonfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_speaker: [0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 0, 1, 2, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 2, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0]\n",
      "client_speaker: [2, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 2, 0, 0, 0, 2, 1, 0, 0, 1, 1, 2, 0, 2, 0, 2, 1, 1, 1, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1, 2, 0, 0, 0, 2, 3, 0, 0, 0, 2, 1, 1, 3, 0, 0, 1, 1, 2, 2, 2, 0, 1, 1, 1, 0, 1, 2, 2, 0, 1, 0, 0, 0, 0, 3, 0, 1, 0, 2, 0, 0, 2, 1, 1, 0, 1, 1, 0, 2, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# Select \"speaker\" who is \"agent\" and \"speaker\" who is \"client\"\n",
    "# agent_speaker = [corpus_multi_dict[0]['speaker'] for corpus_multi_dict in corporea_multi_dict] # The first in talk is considered the 'agent_speaker'\n",
    "agent_speaker = [list(dft[column])[2:].index(sorted(list(dft[column])[2:])[-1]) for column in dft.columns] # The 1st one that talk most is the 'agent_speaker'\n",
    "client_speaker = [list(dft[column])[2:].index(sorted(list(dft[column])[2:])[-2]) for column in dft.columns] # The 2nd one that talk most is the 'client_speaker'\n",
    "print(f'agent_speaker: {agent_speaker}')\n",
    "print(f'client_speaker: {client_speaker}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agent_speaker</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_speaker</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Speaker_0</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Speaker_1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44</td>\n",
       "      <td>62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Speaker_2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>149</td>\n",
       "      <td>65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Speaker_3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               1    2   3   4   5   6   7   8    9   10   11  12  13  14  15   \\\n",
       "agent_speaker    0  NaN   1   1   1   0   1   0  NaN   1  NaN   0   1   1   1   \n",
       "client_speaker   2  NaN   0   0   0   1   0   1  NaN   2  NaN   2   0   0   0   \n",
       "Speaker_0       26  NaN   1   3   2   3   1   5  NaN   1  NaN  41  11   3   8   \n",
       "Speaker_1        0  NaN   4  59   9   2   8   4  NaN   7  NaN   0  23   6  24   \n",
       "Speaker_2        3  NaN   1   0   0   0   1   0  NaN   2  NaN  16   0   1   0   \n",
       "Speaker_3        0  NaN   0   0   0   0   0   0  NaN   0  NaN   0   0   0   0   \n",
       "\n",
       "               16  17   18   19  20  21  22  23  24  25  26  27  28  29  30   \\\n",
       "agent_speaker    1   0  NaN  NaN   0   0   1   1   1   1   1   0   0   2   1   \n",
       "client_speaker   2   1  NaN  NaN   1   1   2   0   2   0   2   1   1   1   0   \n",
       "Speaker_0        0  34  NaN  NaN  46  46   0   1   2   2   1  19  40   2   1   \n",
       "Speaker_1       44   1  NaN  NaN   9   7  11   2  28  12   6   4   4   3   2   \n",
       "Speaker_2       22   0  NaN  NaN   0   0   4   0   5   0   2   0   0  19   0   \n",
       "Speaker_3        0   0  NaN  NaN   0   0   0   0   0   0   0   0   0   0   0   \n",
       "\n",
       "               31   32  33  34  35   36   37   38   39   40  41  42  43  44   \\\n",
       "agent_speaker    1  NaN   0   1   2    2  NaN  NaN  NaN  NaN   0   1   0   1   \n",
       "client_speaker   0  NaN   1   0   0    3  NaN  NaN  NaN  NaN   1   2   1   0   \n",
       "Speaker_0       14  NaN  11   3   3   48  NaN  NaN  NaN  NaN   1   0   6   3   \n",
       "Speaker_1       21  NaN   6   5   2    0  NaN  NaN  NaN  NaN   2  48   6  66   \n",
       "Speaker_2        0  NaN   0   0   6  139  NaN  NaN  NaN  NaN   2   5   1   0   \n",
       "Speaker_3        0  NaN   0   0   0   81  NaN  NaN  NaN  NaN   0   0   0   0   \n",
       "\n",
       "                45  46  47   48  49  50  51  52  53  54   55  56  57  58  59   \\\n",
       "agent_speaker   NaN   0   0  NaN   1   1   1   0   0   1  NaN   1   0   0   1   \n",
       "client_speaker  NaN   2   3  NaN   0   0   2   1   1   3  NaN   0   1   1   2   \n",
       "Speaker_0       NaN  45  27  NaN   8   1   6   9   7   0  NaN   6   9  11   0   \n",
       "Speaker_1       NaN   0   0  NaN  50  43  26   5   2  82  NaN   9   2   1  65   \n",
       "Speaker_2       NaN   6   0  NaN   0   0  14   0   1   0  NaN   0   0   0  17   \n",
       "Speaker_3       NaN   0  10  NaN   0   0   0   0   0  23  NaN   0   0   0   0   \n",
       "\n",
       "                60   61   62  63  64  65  66  67  68  69  70  71   72   73   \\\n",
       "agent_speaker     0    1  NaN   2   0   0   2   0   0   0   1   0  NaN  NaN   \n",
       "client_speaker    2    2  NaN   1   1   1   0   1   2   2   0   1  NaN  NaN   \n",
       "Speaker_0       280   48  NaN   1   6  65   2   9  27  38   2   9  NaN  NaN   \n",
       "Speaker_1         0  103  NaN   4   2  15   2   1   2   0   9   4  NaN  NaN   \n",
       "Speaker_2       149   65  NaN   6   2   0   3   0  14   3   0   0  NaN  NaN   \n",
       "Speaker_3         0    0  NaN   0   0   0   0   0   0   0   0   0  NaN  NaN   \n",
       "\n",
       "                74  75  76   77  78  79  80   81  82  83  84  85  86  87  88   \\\n",
       "agent_speaker   NaN   1   1  NaN   0   1   1  NaN   0   1   0   0   2   0   0   \n",
       "client_speaker  NaN   0   3  NaN   1   0   2  NaN   2   2   1   1   0   1   1   \n",
       "Speaker_0       NaN  10   0  NaN   5   5   0  NaN   8   0  44  29   5  47  26   \n",
       "Speaker_1       NaN  44  62  NaN   1  10  46  NaN   0  84  11  12   2   6   6   \n",
       "Speaker_2       NaN   0   0  NaN   0   4   9  NaN   8  55   0   0  10   3   0   \n",
       "Speaker_3       NaN   0  15  NaN   0   0   0  NaN   0   0   0   0   0   0   0   \n",
       "\n",
       "               89  90  91   92  93  94  95  96  97  98  99   100  \n",
       "agent_speaker    2   1   1  NaN   1   1   0   1   1   0   1  NaN  \n",
       "client_speaker   0   2   0  NaN   0   0   1   0   2   1   2  NaN  \n",
       "Speaker_0        3   1   5  NaN   3   5  40   2   1   7   1  NaN  \n",
       "Speaker_1        2  22  10  NaN   4  10  22   7   8   2   6  NaN  \n",
       "Speaker_2        4   3   2  NaN   2   0   0   1   2   0   2  NaN  \n",
       "Speaker_3        0   0   0  NaN   0   0   0   0   0   0   0  NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(df['Speaker_f'])):\n",
    "    if agent_speaker[i] != client_speaker[i]:\n",
    "        df['Speaker_f'][i+1] = agent_speaker[i]\n",
    "        df['Speaker_s'][i+1] = client_speaker[i]\n",
    "df = df.rename(columns={'Speaker_f':'agent_speaker', 'Speaker_s':'client_speaker'})\n",
    "dft = df.T\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Swap manually the speakers\n",
    "# agent_swap = []\n",
    "# agent_swap = [5,12,14,21,22,24,26,29,32,34,35,41] # agents need to be corrected manually\n",
    "# agent_speaker = list(df['agent_speaker'])\n",
    "# client_speaker = list(df['client_speaker'])\n",
    "# for id in agent_swap:\n",
    "#     aux = agent_speaker[id-1]\n",
    "#     agent_speaker[id-1] = client_speaker[id-1]\n",
    "#     client_speaker[id-1] = aux\n",
    "# df['agent_speaker'] = agent_speaker\n",
    "# df['client_speaker'] = client_speaker\n",
    "# dft = df.T\n",
    "# dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MANUAL EXTRACT DATA\n",
    "# agent_type = ['outbound', 'outbound', 'outbound', 'outbound', 'outbound', 'outbound', 'inbound', 'outbound', 'outbound', 'outbound', 'outbound', 'outbound']\n",
    "# # agent_speaker = [0 for i in range(len(agent_result))]\n",
    "# agent_speaker = [0,0,0,0,0,0,0,1,0,1,0,0]\n",
    "# # agent_result = ['compromiso', 'no_contacto', 'no_contacto', 'compromiso', 'no_contacto', 'reclamo_facturación', 'compromiso', 'reclamo_baja', 'reclamo_baja', 'reclamo_facturación', 'no_contacto', 'compromiso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dictionary of speaker n in multi_dic[i]\n",
    "# n = 1\n",
    "# i = 12\n",
    "# [{word[\"word\"],word[\"word_confidence\"]} for word in corporea_multi_dict[i-1] if word['speaker']==n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medida de riqueza lexica en un texto: \n",
    "$$ R_l = \\frac{\\text{total de palabras únicas}}{\\text{total de palabras}} = \\frac{\\text{longitud del vocabulario}}{\\text{longitud del texto}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, corpus_multi_dict in enumerate(corporea_multi_dict):\n",
    "    audio_metrics[i]['time_voices']             = sum((word['end']-word['begin']) for word in corpus_multi_dict)\n",
    "    audio_metrics[i]['time_speaker_0']          = sum((word['end']-word['begin']) for word in corpus_multi_dict if word['speaker']==agent_speaker[i])\n",
    "    audio_metrics[i]['time_speaker_1']          = sum((word['end']-word['begin']) for word in corpus_multi_dict if word['speaker']==client_speaker[i])\n",
    "    audio_metrics[i]['voices_call_ratio']       = audio_metrics[i]['time_voices']/audio_metrics[i]['time_call'] if audio_metrics[i]['time_call'] != 0 else 0\n",
    "    audio_metrics[i]['agent_call_ratio']        = audio_metrics[i]['time_speaker_0'] /audio_metrics[i]['time_call'] if audio_metrics[i]['time_call'] != 0 else 0\n",
    "    audio_metrics[i]['agent_dominance']         = audio_metrics[i]['time_speaker_0'] /audio_metrics[i]['time_voices'] if audio_metrics[i]['time_voices'] != 0 else 0\n",
    "    audio_metrics[i]['agent_words_sum']         = sum( 1 for word in corpus_multi_dict if word['speaker']==agent_speaker[i])\n",
    "    audio_metrics[i]['client_words_sum']        = sum( 1 for word in corpus_multi_dict if word['speaker']==client_speaker[i])\n",
    "    audio_metrics[i]['agent_words_per_second']  = audio_metrics[i]['agent_words_sum']/audio_metrics[i]['time_speaker_0'] if audio_metrics[i]['time_speaker_0'] != 0 else 0\n",
    "    audio_metrics[i]['client_words_per_second'] = audio_metrics[i]['client_words_sum']/audio_metrics[i]['time_speaker_1'] if audio_metrics[i]['time_speaker_1'] != 0 else 0\n",
    "    audio_metrics[i]['agent_speaker']           = agent_speaker[i]\n",
    "    audio_metrics[i]['client_speaker']          = client_speaker[i]\n",
    "    audio_metrics[i]['agent_name']              = agent_name[i]\n",
    "    audio_metrics[i]['cartera']                 = audio_cartera[i]\n",
    "    audio_metrics[i]['audio_fecha']             = audio_fecha[i]\n",
    "    audio_metrics[i]['audio_hora']              = audio_hora[i]\n",
    "    audio_metrics[i]['rut']                     = rut[i]\n",
    "    audio_metrics[i]['nombre']                  = nombres[i]\n",
    "    audio_metrics[i]['calidad']                 = calidad[i]\n",
    "    # audio_metrics[i]['lexical_wealth ']         = len(set([word['word'] for word in corpus_multi_dict if word['speaker']==agent_speaker[i]]))/len([word['word'] for word in corpus_multi_dict if word['speaker']==agent_speaker[i]]) if len([word['word'] for word in corpus_multi_dict if word['speaker']==agent_speaker[i]]) != 0 else 0\n",
    "    audio_metrics[i]['conversation']            = documents[i]\n",
    "# audio_metrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct dialogs new\n",
    "timings = []\n",
    "dialogs = []\n",
    "conversations = []\n",
    "speakers = []\n",
    "for i, corpus_multi_dict in enumerate(corporea_multi_dict):\n",
    "    begin = 0\n",
    "    end = 0\n",
    "    sentence = ''        \n",
    "    timing = []\n",
    "    dialog = []\n",
    "    conversation = []\n",
    "    speak = []\n",
    "    agent = agent_speaker[i]\n",
    "    client = client_speaker[i]\n",
    "    try:\n",
    "        speaker = corpus_multi_dict[0]['speaker']\n",
    "        for j, word in enumerate(corpus_multi_dict):\n",
    "            if word['speaker'] == speaker:\n",
    "                sentence += f\"{word['word']} \"\n",
    "            else:\n",
    "                end = corpus_multi_dict[j-1]['end']\n",
    "                timing.append([begin,end])\n",
    "                if speaker == agent: label = 'agent'\n",
    "                elif speaker == client: label = 'client'\n",
    "                else: label = 'none'\n",
    "                dialog.append({'file': i+1, 'pos':len(dialog)+1, 'speaker': speaker, 'label': label, 'sentence':sentence[:-1]})\n",
    "                conversation.append(sentence[:-1])\n",
    "                speak.append(speaker)\n",
    "                sentence = f\"{word['word']} \"\n",
    "                speaker = word['speaker']\n",
    "                begin = word['begin']\n",
    "        end = word['end']\n",
    "    except:\n",
    "        pass\n",
    "    timing.append([begin,end])\n",
    "    dialog.append({'file': i+1, 'pos':len(dialog)+1, 'speaker': speaker, 'label': label, 'sentence':sentence[:-1]})\n",
    "    conversation.append(sentence[:-1])\n",
    "    speak.append(speaker)\n",
    "    timings.append(timing)\n",
    "    dialogs.append(dialog)\n",
    "    conversations.append(conversation)\n",
    "    speakers.append(speak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>pos</th>\n",
       "      <th>agent</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hola buenos días es usted nelson habite adis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>sí aló sí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>le damos la bienvenida armó un tenemos atracti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>hola</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file  pos  agent                                           sentence\n",
       "0     1    1      1       hola buenos días es usted nelson habite adis\n",
       "1     1    2      0                                          sí aló sí\n",
       "2     1    3      0  le damos la bienvenida armó un tenemos atracti...\n",
       "3     2    1      0                                                   \n",
       "4     3    1      0                                               hola"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dialogs_trans = {key:[] for key, value in dialogs[0][0].items()}\n",
    "dialogs_trans = {'file':[], 'pos':[], 'agent':[], 'sentence':[]}\n",
    "for dialog in dialogs:\n",
    "    for sentence in dialog:\n",
    "        for key, value in sentence.items():\n",
    "            if key == 'label':\n",
    "                if value == 'agent': dialogs_trans['agent'].append(1)\n",
    "                else: dialogs_trans['agent'].append(0)\n",
    "            elif key != 'speaker': dialogs_trans[key].append(value)\n",
    "\n",
    "dialogs_df = pd.DataFrame()\n",
    "for key, value in dialogs_trans.items():\n",
    "    dialogs_df[key] = value\n",
    "dialogs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "for conversation in conversations:\n",
    "    document = \"\"\n",
    "    for sentence in conversation:\n",
    "        document = f\"{document}{sentence}. \"\n",
    "    document = document[:-1]\n",
    "    documents.append(document)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics data\n",
    "dialogs_df.to_csv(f\"{mypath}/transcripts/results/dialogs.csv\", index=False, encoding='utf8')\n",
    "tosave = {  'audio_metrics'         : audio_metrics,\n",
    "            'corporea'              : corporea,\n",
    "            'corporea_multi'        : corporea_multi,\n",
    "            'corporea_multi_dict'   : corporea_multi_dict,\n",
    "            'dialogs'               : dialogs,\n",
    "            'speakers'              : speakers}\n",
    "for key, data in tosave.items():\n",
    "    with open(f\"{mypath}/transcripts/results/{key}.json\", 'w', encoding='utf8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False)\n",
    "        \n",
    "# save audio_metrics to excel file\n",
    "with open(f\"{mypath}/transcripts/results/audio_metrics.json\", 'r', encoding='utf8') as json_file:\n",
    "    data = json.loads(json_file.read())\n",
    "df_audio_metrics = pd.json_normalize(data)\n",
    "df_audio_metrics.to_excel(f\"{mypath}/transcripts/results/audio_metrics.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # individual save\n",
    "# onlysave = {'audio_metrics': audio_metrics,}\n",
    "# for key, data in onlysave.items():\n",
    "#     with open(f\"{mypath}/transcripts/results/{key}.json\", 'w', encoding='utf8') as json_file:\n",
    "#         json.dump(data, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "key = os.environ['AZURE_KEY']\n",
    "endpoint = os.environ['AZURE_ENDPOINT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "client = authenticate_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SENTIMENT\n",
    "def sentiment_analysis(client, documents=[\"Tuve el mejor día de mi vida. Desearía que hubieras estado ahí.\"]):\n",
    "    sentiments = []\n",
    "    for document in documents:\n",
    "        response = client.analyze_sentiment([document])[0] #Llamada al servicio\n",
    "        sentiments.append(response)\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFICAR\n",
    "def sentiment_analysis_example(client, documents=[\"Tuve el mejor día de mi vida. Desearía que hubieras estado ahí.\"]):\n",
    "    response = client.analyze_sentiment(documents) #Llamada al servicio\n",
    "    print(\"Document Sentiment: {}\".format(response.sentiment))\n",
    "    print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "        response.confidence_scores.positive,\n",
    "        response.confidence_scores.neutral,\n",
    "        response.confidence_scores.negative,\n",
    "    ))\n",
    "    for idx, sentence in enumerate(response.sentences):\n",
    "        print(\"Sentence: {}\".format(sentence.text))\n",
    "        print(\"Sentence {} sentiment: {}\".format(idx+1, sentence.sentiment))\n",
    "        print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "            sentence.confidence_scores.positive,\n",
    "            sentence.confidence_scores.neutral,\n",
    "            sentence.confidence_scores.negative,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = sentiment_analysis(client, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = []\n",
    "for i in range(len(sentiments)):\n",
    "    numbers.append(len(sentiments[i].sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Datos Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traer_deuda(rut = rut[90],codemp = audio_cartera[90],pais = 152)\n",
    "# traer_deuda(rut = '003421657-6',codemp = 'CAT',pais = 152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "def traer_deuda(rut,codemp,pais):\n",
    "    URL = 'https://api-recsaone.recsa.cl/api/malena/cliente-acciones'\n",
    "    PARAMS = {'accion':'traer_deudas',\n",
    "                'rut':rut,\n",
    "                'cod_emp':codemp,\n",
    "                'usuario':'voicebot',\n",
    "                'cod_cli':'',\n",
    "                'pais':pais}\n",
    "    HEADERS = {'api-key':os.environ['API_RECSAONE'],\n",
    "                'Content-Type':'application/json',\n",
    "                'Accept':'application/json'}\n",
    "    r = requests.post(url=URL, params=PARAMS, headers=HEADERS)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "nombres = []\n",
    "for i,r in enumerate(rut):\n",
    "    data = traer_deuda(rut = r,codemp = audio_cartera[i],pais = 152)\n",
    "    if i==50: time.sleep(40)\n",
    "    try:\n",
    "        nombres.append(data['Nombre'])\n",
    "    except:\n",
    "        nombres.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from unicodedata import normalize\n",
    "\n",
    "calidad = []\n",
    "for i,document in enumerate(documents):\n",
    "    if document == '.': calidad.append('Vacio')\n",
    "    elif isinstance(nombres[i],dict): calidad.append('Error API')\n",
    "    else:\n",
    "        mencionado = False\n",
    "        document_normalized = normalize('NFC', re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", normalize( \"NFD\", document), 0, re.I))\n",
    "        for nombre in nombres[i].split():\n",
    "            if nombre.lower() in document_normalized: mencionado = True\n",
    "        if mencionado: calidad.append('Mencionado')\n",
    "        else: calidad.append('No Mencionado')\n",
    "len(calidad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoMencionado_index = [i for i,cal in enumerate(calidad) if cal == 'No Mencionado']\n",
    "# Mencionado_index = [i for i,cal in enumerate(calidad) if cal == 'Mencionado']\n",
    "# print(NoMencionado_index)\n",
    "# print(Mencionado_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoMencionado = [corporea[i] for i in NoMencionado_index]\n",
    "# Mencionado = [corporea[i] for i in Mencionado_index]\n",
    "# NoMencionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docx dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING\n",
    "from docx.shared import RGBColor\n",
    "\n",
    "indent = 1.5\n",
    "for n, conversation in enumerate(conversations):\n",
    "    timing = timings[n]\n",
    "    # sentences = sentiments[n].sentences\n",
    "    document = Document()\n",
    "    docx_name = audio_metrics[n]['file_name']\n",
    "    metric = []\n",
    "    metric.append(f\"rut: {audio_metrics[n]['rut']}\")\n",
    "    metric.append(f\"time_call: {audio_metrics[n]['time_call']:.2f} s\")\n",
    "    metric.append(f\"agent_name: {audio_metrics[n]['agent_name']}\")\n",
    "    metric.append(f\"cartera: {audio_metrics[n]['cartera']}\")\n",
    "    metric.append(f\"agent_dominance: {audio_metrics[n]['agent_dominance']*100:.0f}%\")\n",
    "    metric.append(f\"agent_words_per_second: {audio_metrics[n]['agent_words_per_second']:.2f} wps\")\n",
    "    metric.append(f\"client_words_per_second: {audio_metrics[n]['client_words_per_second']:.2f} wps\")\n",
    "    # metric.append(f\"conversation sentiment: {sentiments[n].sentiment}\")\n",
    "    # metric.append(f\"overall scores: positive={sentiments[n].confidence_scores.positive:.2f}; neutral={sentiments[n].confidence_scores.neutral:.2f}; negative={sentiments[n].confidence_scores.negative:.2f}\")\n",
    "\n",
    "    paragraph = document.add_heading(level=0)\n",
    "    paragraph_format = paragraph.paragraph_format\n",
    "    paragraph_format.space_after = Pt(3)\n",
    "    run = paragraph.add_run(f'{docx_name}')\n",
    "    run.font.size = Pt(16)\n",
    "\n",
    "    for m in metric:\n",
    "        paragraph = document.add_paragraph()\n",
    "        paragraph_format = paragraph.paragraph_format\n",
    "        paragraph.add_run(str(m)).font.size = Pt(9)\n",
    "        paragraph_format.space_after = Pt(0)\n",
    "    paragraph_format.space_after = Pt(9)\n",
    "\n",
    "    begin = 0\n",
    "    left = True\n",
    "    for s in speakers[n]:\n",
    "        if s == agent_speaker[n] or s == client_speaker[n]:\n",
    "            if s == agent_speaker[n]:\n",
    "                left = False\n",
    "            break\n",
    "    for i, sentence in enumerate(conversation):\n",
    "        if speakers[n][i] == agent_speaker[n] or speakers[n][i] == client_speaker[n]:\n",
    "            paragraph = document.add_paragraph()\n",
    "            # timestamp = f\"{sentences[i].sentiment} | pos:{sentences[i].confidence_scores.positive:.2f} | neu:{sentences[i].confidence_scores.neutral:.2f} | neg:{sentences[i].confidence_scores.negative:.2f} | [{timing[i][0]:.1f}, {timing[i][1]:.1f}]s\"\n",
    "            timestamp = f\"[{timing[i][0]:.1f}, {timing[i][1]:.1f}]s\"\n",
    "            if left:\n",
    "                paragraph.add_run(f'{sentence}').font.color.rgb = RGBColor(0,176,80)\n",
    "                paragraph.paragraph_format.right_indent = Inches(indent)\n",
    "                paragraph.paragraph_format.space_after = Pt(0)\n",
    "                paragraph = document.add_paragraph()\n",
    "                paragraph.add_run(f'{i+1}. {timestamp}').font.size = Pt(7)\n",
    "                paragraph.paragraph_format.right_indent = Inches(indent)\n",
    "            else:\n",
    "                paragraph.add_run(f'{sentence}').font.color.rgb = RGBColor(0,112,192)\n",
    "                paragraph.paragraph_format.left_indent = Inches(indent)\n",
    "                paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
    "                paragraph.paragraph_format.space_after = Pt(0)\n",
    "                paragraph = document.add_paragraph()\n",
    "                paragraph.add_run(f'{i+1}. {timestamp}').font.size = Pt(7)\n",
    "                paragraph.paragraph_format.left_indent = Inches(indent)\n",
    "                paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
    "            paragraph.paragraph_format.space_after = Pt(6)\n",
    "            left = not left\n",
    "    document.add_page_break()\n",
    "    document.save(f\"{mypath}/transcripts/docx/{docx_name}.docx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
